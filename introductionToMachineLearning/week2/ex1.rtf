{\rtf1\ansi\ansicpg1252\cocoartf1038\cocoasubrtf360
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww9000\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\ql\qnatural\pardirnatural

\f0\fs24 \cf0 Introduction to Machine Learning week2, ex1\
\
a) The transformation in the case where a term appears in only one document the tf is multiplied by a large factor, where as in the case of all documents it is multiplied by log 1, which actually ends up taking down the value of tf, definitely not increasing it like in the case of a lower df.\
\
b) The overall effect is that a term frequency of a word in a document is increased based on the inverse document frequency, the bigger the increase the less documents contain this term. The increase is logarithmic so tf still has the main role. The purpose of using this transformation is based on the fact that many words are common in various languages, so finding relevant or interesting words is made easier by this transformation. \
\
c) If we for example want to look at the frequency a single customer has bought some product from a supermarket, we might want to do a similar transformation to take into account some very ordinarily bought products that can be found commonly from a majority of customers. An idf transformation here would favor those purchases that are more unique for the user in question, and we would get a better idea of what kind of "user-specific" products are interesting to this person.}
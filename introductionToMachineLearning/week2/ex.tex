\documentclass{article}
\usepackage{mathptmx}
\title{Introduction to Machine Learning, week 2}
\author{Tony Kovanen}
\begin{document}

\section{ex 1}
\subsection{a}
The transformation in the case where a term appears in only one document the tf is multiplied by a large factor, where as in the case of all documents it is multiplied by log 1, which actually ends up taking down the value of tf, definitely not increasing it like in the case of a lower df.

\subsection{b}
The overall effect is that a term frequency of a word in a document is increased based on the inverse document frequency, the bigger the increase the less documents contain this term. The increase is logarithmic so tf still has the main role. The purpose of using this transformation is based on the fact that many words are common in various languages, so finding relevant or interesting words is made easier by this transformation. 

\subsection{c}
If we for example want to look at the frequency a single customer has bought some product from a supermarket, we might want to do a similar transformation to take into account some very ordinarily bought products that can be found commonly from a majority of customers. An idf transformation here would favor those purchases that are more unique for the user in question, and we would get a better idea of what kind of "user-specific" products are interesting to this person.

\section{ex 2}
\subsection{a}
Cosine similarity has a range of [-1, 1]. The values of $cos(x, y)$ will range from $0$ towards $1$ the larger the dot product of $x$ and $y$ is if nonnegative, and from $0$ to $-1$ the more negative the product is.

\subsection{b}
No. Since the cosine similarity measures the cosine of the angle between the two vectors, $x$ and $y$, the vectors need not be exactly the same for the cosine similarity $cos(x,y)$ to be equal to $1$. Instead they just need to be parallel (can even point in opposite direction). However, if the cosine similarity $cos(x,y) = 1$, then we know that vectors $x$ and $y$ differ from each other only by a constant factor.

\subsection{c}
If we look at how cosine similarity is defined, it is basically the dot product of two given vectors, divided by the the product of the norms of the two vectors: $\frac{x^Ty}{||x|| ||y||}$. Correlation is defined as $\frac{cov(x, y)}{\sigma_x \sigma_y}$, where $\sigma$ stands for standard deviation, and $cov$ is the covariance between the given vectors. We can see by the definition of $\sigma$ and the definition of $cov$ that correlation is exactly the same as cosine similarity, but the data is "centered", which means that the mean of each column/feature is is deducted from each data point. If we do this as a preprocessing step to our data, the cosine similarity of this preprocessed data is exactly the same as the correlation of the original data. 

\subsection{d}
Cosine similarity in the case of a norm of 1 can be written in the form $x^T y$, which on a lower level can be written as $\sum_{i=1}^{n} x_i  y_i$. Euclidean distance is defined as $d(x,y) = \sum_{i=1}^{n} (x_i - y_i)^2$. As we can see, where in cosine similarity we would sum the product of each term in input x and y, in euclidean distance the second power of their subtraction. 

\subsection{e}

\section{ex 3}
\subsection{a}
Let $\mathcal{A}$ be a set of more than one objects. The proximity among the objects in this set can be defined in many differenet ways. One of these is the maximum of the distance between two points in the set. Another might be the mean distance between any two points in $\mathcal{A]$. Here distance refers to some measure of distance for example euclidean distance. 

\subsection{b}
The distance between two sets of points in euclidean space could be intuitively defined as the distance between the medoids (centers of mass) of these two sets. Another approach would be to define the distance as the minimum distance between any two points $a$ and $b$, where $a\in A$ and $b\in B$.

\subsection{c}
As the minimum of all the euclidean distances between points $a\in A$ and $b\in B$.

\end{document}
